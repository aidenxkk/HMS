# HMS - Hierarchical Model Structures
### Project Summary 
This project investigates how model architecture affects performance when privacy and robustness constraints are imposed. Scaling laws in machine learning suggest that larger models generally achieve better accuracy, but this advantage often disappears when training with differential privacy or adversarial defenses. To explore alternatives, this work compares a conventional monolithic classifier with a hierarchical stepwise structure composed of three smaller sub-models. The hierarchical model first performs coarse classification and then refines predictions through specialized fine classifiers. Both structures are trained and evaluated under two settings: differentially private stochastic gradient descent (DP-SGD) and adversarial (minimax) training. Experiments on CIFAR-10 and CIFAR-100 measure accuracy, robustness, and privacy trade-offs under equal computational budgets. The study aims to determine whether modular, structured models can maintain or improve accuracy and stability compared to single large networks, offering insights into efficient and privacy-preserving model design.
